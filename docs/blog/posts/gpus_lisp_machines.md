---
draft: true
date: 2025-01-28
authors:
  - dfellis
categories:
  - PL Design
  - Hardware Architecture
  - DevExp
---

# The Return of the Lisp Machines: The Modern GPU

In the late 70s and early 80s there were computers designed for a single programming language with a lot of focus on a particular computing goal: the Lisp Machines. Modern GPUs share a surprising number of similarities with these Lisp Machines both in structure and application.

## History doesn't Repeat, but it Rhymes

<!-- more -->

The [Lisp Machines](https://en.wikipedia.org/wiki/Lisp_machine) are from before my time, having started a few years before I was born and having died out with the ascent of personal computing technologies such as Windows 95 and the Nintendo 64. I do not have direct experience with them, but I have written Lisp in a commercial setting during an internship with a semiconductor company while in grad school, and a variety of experience from 68k assembly in college to various modern languages (Java, Javascript, Python, and Rust in particular) in large scale backend deployments at Silicon Valley startups (Uber, CarDash, and Alan Tech), and lots in between. So this is a perspective on Lisp machines from the outside looking in, but with enough to back it up that it hopefully isn't too distorted of a perspective on things.

There is no direct connection between the Lisp Machines of 40-50 years ago and the GPUs of today, but the technical design, primary use-cases, and target audience of both are strikingly similar, and that it has developed out of a subsystem of the same machines that economically demolished the Lisp Machines has a poetic death-and-rebirth symbolism to it.... Do I hear cackling coming from the old Symbolics building?

## Architectural Similarities between Lisp Machines and Modern GPUs

Some later "Lisp Machines" were daughter cards added to personal computers, such as the [MacIvory for the Apple \]\[](http://fare.tunes.org/LispM.html), that look very similar in concept to GPU cards in modern PCs. This does bring up an interesting "what if" to think about, if they had persisted another 10 years and directly competed with the early GPUs, but if that were it they would just as similarly be compared to sound cards, network cards, or anything that had a sub-processor dedicated to a narrower task. No, the similarities between them are deeper than that.

How can a computer be a "Lisp machine" anyway? If it's Turing complete, it can run anything, and you need to compile a high-level language's text representation into something the machine can consume. These Lisp machines are no different, having a [compiler](https://hanshuebner.github.io/lmman/compil.xml#compiler-chapter) to generate machine code, though this is also directly exposed within the Lisp interpreter as a way to optimize performance of the code you are directly working on, blurring the boundaries between development and execution in the late 70s that [the Python community recreated decades later](https://jupyter.org/), but that's the topic for another article in another time.


Lisp Machines have many [foundational Lisp functions implemented as CPU instructions](https://hanshuebner.github.io/lmman/code.xml#Introduction-section) that are labeled `(misc)` by the built-in disassembler. But modern ARM chips [have an instruction explicitly for Javascript's funky `Number` type](https://developer.arm.com/documentation/dui0801/h/A64-Floating-point-Instructions/FJCVTZS) to speed up web browsers to catch up with x86-64 in the real world, so language-specific instructions doesn't seem like a solid way to distinguish them from their PC brethren. What then architecturally makes a Lisp Machine a Lisp Machine?

I would argue that it is the *lack* of ABI stability that the Lisp Machines exhibited. [Even from the same company in the same series of Lisp Machines](https://en.wikipedia.org/wiki/Lisp_machine#Initial_development) core elements of the ABI such as the byte length of [CPU words](https://en.wikipedia.org/wiki/Word_(computer_architecture)) would change, let alone the instruction set they run. The stable API of the machines was the *lisp language itself*. This was text-based and largely cross compatible between the various Lisp Machines, even from different companies. You could not distribute binaries of Lisp Machine applications, you *had* to distribute source code, because the binary, if you even bothered to access it, would differ wildly between machines.

If you've played a modern PC game, you may have noticed while the game is starting it saying "Compiling Shaders" or "Compiling Vulkan Shaders." (Or worse, noticed some modern games stutter at random points that would have run without a hitch on older games.) These shaders are similar in many ways to the Lisp that ran on Lisp machines, though the shader languages ([and there are a few](https://en.wikipedia.org/wiki/Shading_language)) are an almost abhorrently anti-Lisp requiring mutable, sometimes hidden, state to even function.

They represent the units of execution the GPUs are tasked with and must be distributed as source code because the GPUs from different manufacturers and between generations from the same manufacturer have different ABIs -- [the GPU drivers include a shader compiler](https://developer.nvidia.com/docs/drive/drive-os/6.0.8.1/public/drive-os-linux-sdk/common/topics/graphics_content/GLSLCShaderProgramCompiler1.html#ariaid-title1) that the game must invoke to translate their shader code into what the GPU can run. Well-written games compile them at startup, and potentially cache them for your particular GPU + driver version (because the driver may also [update the firmware the GPU runs](https://nvidia.custhelp.com/app/answers/detail/a_id/5411/~/nvidia-gpu-uefi-firmware-update-tool) so old compiler output is no longer valid) so they only have to be compiled on the first start of the game and never again, while lesser games lazily evaluate them as needed, and this can cause a stutter in frame generation while it is waiting on the necessary code to be run to actually be available to run.

## Use-cases for GPUs and Lisp Machines

As GPUs grew in capability from a [fixed-function pipeline](https://en.wikipedia.org/wiki/Fixed-function) that could only be configured with parameters/data fed to it by the GPU developers to more general purpose parallel compute tools starting with [vertex and pixel shaders](https://en.wikipedia.org/wiki/Shader#Types), wholly generalized [compute shaders](https://en.wikipedia.org/wiki/Shader#Compute_shaders) came about to allow for non-graphical tasks to be computed on the GPU.

This [GPGPU](https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units) capability is what has enabled things like [Bitcoin mining](https://github.com/RainbowMiner/RainbowMiner), [LLMs](https://github.com/ggerganov/llama.cpp), [Stable Diffusion](https://github.com/Stability-AI/StableDiffusion), and [other AI applications such as medical research](https://www.nvidia.com/en-us/industries/healthcare-life-sciences/) that have appeared in the past decade.

Combine this with the original purpose of GPUs (3D rendering), and you get [many of the same use-cases](https://en.wikipedia.org/wiki/Lisp_machine#Applications) for Lisp Machines. Instead of [transformers](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)) it was [expert systems](https://en.wikipedia.org/wiki/Expert_system) and instead of [ray tracing](https://www.amd.com/en/products/graphics/software/radeon-prorender.html) for rendering it was... [ray tracing](https://en.wikipedia.org/wiki/N-World#Features). (This also shows an interesting link between the two: the 3D assets to eventually be rendered in real time on GPUs were modeled using tools originally written for Lisp Machine hardware, including big ones like Mario 64 and Final Fantasy 7.) The expert system connection also bridges us to the financial industry, as well, [as they were apparently used by American Express for fraud detection](https://danluu.com/symbolics-lisp-machines/), bringing us full-circle with modern GPU use-cases.

There's a common thread in all of these: they're either at the edge of what we can compute in a "reasonable" amount of time with the algorithms that have been developed/discovered at that point in time, or they're problems that are massively parallelizable and there is a desire to minimize the amount of hardware thrown at the problem (or both). This helps us constrain who the audience for Lisp Machines was and currently is for GPUs. Specifically the two major elements of that audience from financial and technical perspectives.

## It's the Finance and Tech Bros Again?

First, I consider the "Tech Bro" and "Finance Bro" stereotype mostly unhelpful and often founded in a sense of jealousy. I'm closer to a stereotypical nerd, but I worked with men (and a few women, shoutout to Brophia, she knows who she is ðŸ˜‰) who worked out, partied on the weekends, and were also razor-sharp, kind to their peers, and seemingly in control of their life to a degree that felt impossible. They're great to work with and are unfairly lumped in with those who I *think* the "Bro" stereotype is levvied at: those who seem to coast through life doing what they want with the confidence that they can do anything and using social connections to erase their mistakes from the record. It is *that* collection of "Bros", both the good and the bad, that is the audience of GPUs now and Lisp Machines then.

Lisp running on Lisp Machines and GPU Shaders on the GPU are both high-level abstractions on the hardware, but the hardware is meant for high performance code that deeply understands the hardware to extract the most amount of performance out of it possible, but the hardware is highly variable from vendor to vendor, even generation to generation in the same vendor, and both the hardware and the compiler may have bugs or idiosyncrasies to work around. To write code for that environment requires a depth of knowledge, a confidence that you can adapt, and energy to persevere that is most common in young, smart professionals.

This also limits the developer audience to those willing or able to keep up with changes to both the hardware, compiler, and language in order to remain a viable candidate for employment in that space, rarefying the hiring pool and raising salaries, further driving potential jealousy and stroking egos.

And the projects themselves are similarly bold: experimental AI applied to the Defense or Finance or Medical industries that has a high risk of failure but a very high reward if successful requires Venture Capital style financial bros behind it, or [big budget Hollywood funding](https://en.wikipedia.org/wiki/Symbolics#Movies) (which is another kind of VC, where the shape of the product is known but the details fuzzy and its lifetime short). I won't comment much more on that as it's outside of my wheelhouse, but there's likely similar social implications within the finance industry.

## Is the Past set to Repeat?

The Lisp Machines collapsed as a business and a technology. Some of this is due to the AI bubble of the 80s popping and a lot of funding that was going into Lisp Machines drying up, and [as of the time of writing this there are signs the same is about to happen to the current AI hype cycle](https://finance.yahoo.com/news/china-deepseek-why-freaking-ai-080636212.html). Some of this was mistakes and fumbles by the Lisp Machine businesses, as [admitted by insiders](https://danluu.com/symbolics-lisp-machines/#why-did-symbolics-fail) at the time.

But GPUs are unlikely to go away in the same way Lisp Machines did. They have cemented their place in real-time rendering for about 30 years, so even if we enter [another AI Winter](https://en.wikipedia.org/wiki/AI_winter) and [Dogecoin](https://en.wikipedia.org/wiki/Dogecoin) falls back to the Earth, there will still be an [almost half-a-trillion-dollars-per-year market](https://www.statista.com/topics/868/video-games/) for it.

However, if these non-graphics uses continue to proliferate, there *will* be market pressure to commodify GPUs and democratize GPGPU programming, to reduce the [economic rents](https://en.wikipedia.org/wiki/Economic_rent) that [nVidia](https://finance.yahoo.com/quote/NVDA/key-statistics/) and the [CUDA-wielding developers](https://developer.nvidia.com/cuda-toolkit) receive in sales and wages, respectively. This is not great news for nVidia and existing GPGPU developers, and the new power available to developers is unlikely to raise their salaries, but it should allow more diverse groups of developers to tackle problems that would have required significant funding before to achieve, perhaps as their own startup, which should be a net positive for the economy as a whole. As we know, [the needs of the many outweigh the needs of the few, or the one](http://quotes4u.org/star-trek/the-needs-of-the-many-outweigh-the-needs-of-the-few/), and something that shaves billions off the top of corporations and gives regular people another tool to pull themselves up is still a net good and one I wish to pursue (your actions all have a political element, even if you don't realize it; best to make sure they align with your values).

I want to democratize GPGPU development. If there is going to be [Artificial General Intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence) I'd rather it be born to an individual rather than a corporation to minimize the chance that [corporate psychopathy](https://en.wikipedia.org/wiki/Psychopathy_in_the_workplace) is passed on. ðŸ«¤ Thankfully this goal is *primarily* technical in nature (with a dash of psychology to get the developer experience in a good place), and we can analyze the socio-technical aspects of the Lisp Machines' downfall.

## Worse is Better, Again?

While researching this, I realized that the famous [Worse is Better](https://en.wikipedia.org/wiki/Worse_is_better) critique of Lisp in favor of Unix was *about* the Lisp Machines, and the originator of the idea is [ambivalent about it](https://www.dreamsongs.com/WorseIsBetter.html), not wanting to accept the very line of reasoning he started, perhaps because of how vehemently his colleagues reacted to it, and how it was accidentally one of the factors in the loss of confidence in the Lisp Machines he championed.

In his defense (and desire for "Worse is Better" to be wrong), "Better is Better" *can* be true, but only if your better system *never* leaks its underlying abstractions, where *never* is technically a sliding scale, but the odds of the abstraction leaking should be very low. Computing hardware is a good example here where [cosmic rays could flip the bit of a computation](https://stackoverflow.com/a/2580963) but the odds are very low so we just continue on as if it doesn't really happen (or bolt on a bit of error checking if we need to be *really* sure).

While the Worse is Better essay focused on the analysis paralysis elements of the Lisp Machine's approach, I think that they were not better *enough* to fully abstract away the hardware is the largest reason for their demise. When something goes wrong with my code, I first assume that I am the dumbass, because I probably am, and only after that do I dig deeper into the abstraction that I am working within.

If the abstraction that I am working within is more straightforward to reason about, either because it is simpler (C) so I can mostly understand the layer below it, or modular (shell + UNIX commands) so I can just disable chunks of it to narrow down the scope of what I need to look at, it's easier to figure out where the error is coming from.

Unix was clear about its abstractions: the shell, the C language, and the CPU architecture were obvious rather than obfuscated. The Lisp Machines ran the Lisp code on a wide variety of architectures and used Lisp functions that could be changed dynamically at runtime, that could be run via an interpreter or compiled to native code or swapped out with a hardware-specific implementation, and was debugged with an abstract assembly language that wasn't actually what any of the machines ran and may or may not have actually been an intermediate representation the compilation path used.

You would have to be pretty fearless to swim in such murky waters even if you were reassured that there weren't any sharks in there. Unix definitely had sharks, but the waters were pretty clear so you could convince yourself that you could avoid them. ðŸ˜‰

(To take this analogy to its conclusion, the hardware is like a frozen ocean you're walking across. It's *very* unlikely that it'll ever crack and swallow you up, but if it does, there's almost nothing to do about it as you're going to get hypothermia and die in that case.)

This is one other point where modern GPUs share a lot of similarities with Lisp Machines, but also some differences that affect the outcome. They evolved from fixed-function rendering tools that were *clearly* worse than the ray tracing the Lisp Machines did for films and accrued functionality over time, just like Unix, and recently gained ray tracing support as an actual top-line feature. But they also all have very wide-ranging architectures under the hood, rather than the more uniform architectures of the `x86-64`, `arm64`, and `riscv` CPU lines. Many competing companies implementing a singular architecture with some performance and occasionally instruction extensions as the differentiating factor, but mostly being a uniform abstraction that multiple languages can target.

The proprietary nature of the GPU architectures is the wedge that nVidia's CUDA fit into; exposing GPU functionality in a *slightly* more ergonomic way for GPGPU that can only work on their own hardware.

So the immediate reaction is that GPGPU will go the same way the Lisp Machines went: the "normal" PC CPU caught up and beat it in performance and it no longer made any sense for using a separate computer or a daughter card to accomplish the same thing in a strange computing environment with different fundamentals, and so we just need AMD to make a super [Threadripper](https://en.wikipedia.org/wiki/Threadripper) with a ridiculous number of CPU cores and let us accomplish the same tasks GPGPU can do without data jumping back and forth across the PCI Express Bus and compiling shaders with weird structures and computing constraints.

This *may* be what eventually happens because of the [diminishing returns on single-threaded CPU performance](https://www.cpubenchmark.net/year-on-year.html) showing single-threaded perf gains of only 2x over 12 years -- the CPU is going to become more GPU-like over time and may claw back much of the GPGPU work being done these days.

But there's also a [tail wags the dog](https://www.merriam-webster.com/dictionary/the%20tail%20wagging%20the%20dog) situation with GPUs that doesn't *require* GPGPU to die for "regular" developers to be able to take advantage of GPUs. Intel recently re-entered the competitive GPU market with their Arc GPUs. The entry was *not* successful at first, mostly because games tested on nVidia and AMD hardware sometimes didn't work right on Intel's hardware, sometimes because of quirks or mistakes on Intel's part, but sometimes because of technically legal differences in how the shader code was compiled due to [undefined behavior](https://en.wikipedia.org/wiki/Undefined_behavior) in improper usage of the shading language that happened to work on nVidia and AMD hardware but not Intel hardware. In order to compete in the market, [Intel had to add workarounds to buggy game code that they couldn't change and make their architecture behave more like the competition](https://gamersnexus.net/gpus/one-year-later-intel-arc-gpu-drivers-bugs-huge-improvements), and this has led to a [successful launch of their second generation GPUs](https://www.tomshardware.com/pc-components/gpus/intel-arc-b580-review-the-new-usd249-gpu-champion-has-arrived).

## Better Enough is Better

The leaks in the abstraction are being plugged up, though it's still [very leaky if you're after performance and not just consistent computation](https://www.nuss-and-bolts.com/p/optimizing-a-webgpu-matmul-kernel) but this particular "leakiness" is relatively consistent across GPUs. It's more akin to writing raw assembly language for a CPU and not realizing that you should use bitshifting when you need to divide by a power of 2 instead of using the divide opcode.

And being consistent but difficult is something we already know how to deal with on the CPU side. Modern C is more of a high-level language than you might at first imagine because the big C compilers (GCC, Clang, MSVC, and kinda ICC) all have optimization passes where they detect certain patterns in your code and generate more efficient assembly that is equivalent to what you wrote but much faster. At first these optimizations were very finicky, where slight deviations in the pattern could cause the optimization to go away, and there is still some of that from time-to-time, but less often than before, and the C language has been allowed to become "lazy" and literally not have any way to express some of the CPU features the compiler can target in the standard (there is the quasi-standard [`x86intrin.h`](https://clang.llvm.org/doxygen/x86intrin_8h_source.html) for `x86-64` processors, [`arm64_neon.h`](https://developer.arm.com/architectures/instruction-sets/intrinsics/) for ARM64 intrinsics, and [`intrin.h`](https://learn.microsoft.com/en-us/cpp/intrinsics/intrinsics-available-on-all-architectures?view=msvc-170) for the subset of intrinsics across `x86`, `x86-64`, `arm`, and `arm64` architectures, but these are nonstandard and not guaranteed to exactly match across compilers).

This "laziness" however, has also helped keep the bar for writing performant C code low, and even for "regular" non-SIMD code the performance you get is better than what earlier compilers would have given you, with things like [loop unrolling](https://en.wikipedia.org/wiki/Loop_unrolling), the aforementioned equivalent math or boolean operation substitution when provably faster, [compile-time function evaluation](https://en.wikipedia.org/wiki/Compile-time_function_execution), and [interprocedural optimization](https://en.wikipedia.org/wiki/Interprocedural_optimization) to automatically inline functions in hot loops even if from a separate library.

## The "Better Enough"

As the WebGPU optimization post noted, [WebGPU shader compilers don't do any of these optimizations](https://www.nuss-and-bolts.com/i/150233403/kernel-unrolling) and you need to unroll your loops yourself. This is likely due to game developers inserting shader compilation during frame generation so the shader compiler developers are pressured to write the *fastest* compiler they can, not the most optimal one. But there's nothing that stops us from writing our own optimizing compiler on top of [WGSL](https://www.w3.org/TR/WGSL/).

That's only one part of lowering the barrier to entry to GPGPU programming, though. More important than that, the ergonomics of the GPU compute need to be much better. GPU compute should "feel" not much different to CPU programming, *but* it should have a little bit of friction on loading data into and out of the GPU, as that is the biggest bottleneck in the system. Explicitly converting an array of CPU data into a buffer of GPU data and a separate but similar set of types in the type system that you mostly can't mix together but have near identical semantic behavior within the language would go a long way there.

This is an explicit objective of the Alan programming language. The focus at the moment is on getting the GPGPU integration and type system into the mostly-ergonomic, discouraging-bad-usage state, and then adding an optimizing compiler in afterwards to raise "idiomatic Alan" into "optimized WGSL."

The language will work on any CPU architecture (testing on x86-64 and ARM64, was testing on RISC-V until I melted my test board), any GPU (testing on nVidia, AMD, Intel, Apple, and Broadcom GPUs), and any operating environment (testing on Windows, MacOS, Linux, and Chromium right now. Want to expand to BSD, Android, and iOS eventually), while looking similar-ish to Typescript for easier adoption. It's *already* much nicer to work with than the WebGPU API and capable of 90% of the same things.

The unfortunately poor management of Symbolics doomed it, but that wouldn't have doomed the entire Lisp Machine concept. What doomed them all was an lack of desire or inability to make them more accessible to "regular" developers, increasing the supply of talent, some of whom would be hired by the existing demand that they had demonstrated, and some of whom would become new sources of demand as they came up with new and interesting ways to use it. By raising the bar on who could use it by raising the stakes so high on its use, they strangled the ecosystem before it could self-support without outside funding propping it up.

Alan will provide a GPGPU programming language for the rest of us, and it will ride on the coattails of the backlog of PC games and the WebGPU initiative to normalize the behavior across GPUs and make the leaky abstraction, if not less leaky, then less murky, so it is less difficult to debug when something goes wrong.
